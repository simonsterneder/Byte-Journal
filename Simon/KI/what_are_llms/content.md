---
title: "Was sind eigentlich LLMs â€“ und warum reden gerade alle darÃ¼ber?"
author: "Simon Sterneder"
date: 2025-05-19
reading_time: "3 min"
---

# Was sind eigentlich LLMs â€“ und warum reden gerade alle darÃ¼ber?

**L**arge **L**anguage **M**odels wie ChatGPT sind in kÃ¼rzester Zeit zu einem festen Bestandteil meines Arbeitsalltags geworden. Als Entwickler nutze ich sie inzwischen regelmÃ¤ÃŸig â€“ beim Debugging, beim Refactoring, zum Schreiben von Dokumentation oder um neue Frameworks schneller zu verstehen. Was vor Kurzem noch wie ein spannendes Experiment wirkte, ist heute ein praktisches Werkzeug, das mir Zeit spart, Ideen liefert und manchmal sogar neue DenkansÃ¤tze erÃ¶ffnet.

Und ich bin damit nicht allein. In BÃ¼ros, Schulen und Medien sind LLMs wie ChatGPT, Claude oder Gemini derzeit in aller Munde. Die einen feiern sie als Durchbruch in Sachen ProduktivitÃ¤t und KreativitÃ¤t, die anderen warnen vor Fehlinformationen, Urheberrechtsproblemen oder ethischen Grauzonen.

In diesem Artikel will ich verstÃ¤ndlich erklÃ¤ren:
- was LLMs eigentlich sind â€“ und wie sie funktionieren,
- wie sie sich von anderen KI-Systemen unterscheiden,
- wo sie aktuell sinnvoll eingesetzt werden,
- welche Risiken und Herausforderungen es gibt,
- und was in Zukunft technisch und gesellschaftlich auf uns zukommt.

Egal ob du selbst entwickelst, dich fÃ¼r KI interessierst oder einfach verstehen willst, was hinter dem Hype steckt â€“ hier bekommst du einen kompakten Ãœberblick.

## Inhalt
1. KI kurz erklÃ¤rt â€“ und wo LLMs ins Bild passen
2. LLMs im Alltag â€“ was sie kÃ¶nnen und wie sie wirken
3. Wie LLMs entstanden sind â€“ ein Blick zurÃ¼ck
4. Wie LLMs unter der Haube funktionieren
5. Ãœberblick: Die wichtigsten LLMs im Vergleich
6. LLMs im echten Leben â€“ was schon geht
7. Risiken & Grenzen
8. Regulierung & Verantwortung
9. Wohin geht die Reise? Drei Zukunftsszenarien

## 1. KI kurz erklÃ¤rt â€“ und wo LLMs ins Bild passen
Wenn heute von â€KIâ€œ die Rede ist, denken viele sofort an Chatbots oder Bildgeneratoren. Dabei ist KÃ¼nstliche Intelligenz (KI) ein deutlich breiteres Feld. Grob gesagt geht es darum, Maschinen mit FÃ¤higkeiten auszustatten, die wir sonst mit menschlicher Intelligenz verbinden: Lernen, ProblemlÃ¶sen, Muster erkennen, Entscheidungen treffen.

LLMs sind ein Teilbereich dieser groÃŸen KI-Welt â€“ genauer gesagt gehÃ¶ren sie zur sogenannten "generativen KI", also zu Systemen, die Inhalte erzeugen kÃ¶nnen: Texte, Bilder, Code oder sogar Musik. Doch bevor wir uns LLMs im Detail anschauen, lohnt ein kurzer Blick aufs groÃŸe Ganze.

### KI ist nicht gleich KI: Ein kurzer Ãœberblick

Neben Sprachmodellen wie ChatGPT gibt es viele weitere Arten von KI-Systemen. Einige Beispiele:

**Computer Vision:** Hier gehtâ€™s um das Erkennen und Verstehen von Bildern oder Videos. Klassische Anwendungen sind Gesichtserkennung oder Objektdetektion in der Industrie.

**Reinforcement Learning:** Diese Technik bringt Maschinen bei, durch Versuch und Irrtum besser zu werden â€“ Ã¤hnlich wie ein Kind, das Fahrradfahren lernt.

**Symbolische KI & regelbasierte Systeme:** Diese â€klassischenâ€œ KI-AnsÃ¤tze basieren nicht auf Daten, sondern auf festgelegten Regeln.

### Wo LLMs ins Bild passen

LLMs (Large Language Models) gehÃ¶ren zum Natural Language Processing (NLP) â€“ also zur Verarbeitung natÃ¼rlicher Sprache. Anders als frÃ¼here NLP-Systeme, die oft stark regelbasiert oder auf spezifische Aufgaben trainiert waren, kÃ¶nnen moderne LLMs flexibel auf unterschiedlichste Eingaben reagieren.

Ihr Erfolg beruht auf einem zentralen Fortschritt: Statt Sprache Wort fÃ¼r Wort zu verstehen, lernen sie auf Basis riesiger Textmengen, Wahrscheinlichkeiten fÃ¼r die nÃ¤chsten WÃ¶rter vorherzusagen â€“ ein Ansatz, der erstaunlich gut funktioniert.

## 2. LLMs im Alltag â€“ was sie kÃ¶nnen und wie sie wirken

LLMs sind erstaunlich vielseitig. Ein und dasselbe Modell kann auf vÃ¶llig unterschiedliche Aufgaben reagieren â€“ je nachdem, wie es angesprochen wird. Hier ein paar Beispiele aus dem Alltag:

**Als Entwicklerhilfe:** Debugging, Code-Optimierung, Dokumentation, Framework-Training

**Als Sprachlehrer:** ErklÃ¤rungen, BeispielsÃ¤tze, Vokabeltraining

**Als Recherchetool:** Faktenchecks, ThemenÃ¼berblicke, Zusammenfassungen

**Als Ideengeber:** StrukturentwÃ¼rfe, Schreibimpulse, NamensvorschlÃ¤ge

Der SchlÃ¼ssel ist: LLMs brauchen keine vordefinierten Befehle. Stattdessen reagieren sie flexibel auf Spracheingaben â€“ das sogenannte Prompting.

### Wie das funktioniert â€“ ein kurzes Beispiel

Technisch basiert ein LLM auf einem Transformer-Modell, das Muster erkennt und sinnvolle Fortsetzungen berechnet. Sprache wird in Tokens zerlegt, in Zahlen umgewandelt und dann statistisch bewertet.

**Beispiel:** Ein Prompt â€“ viele Talente

**Prompt:** <br>
<code> "ErklÃ¤re mir, wie Merge Sort funktioniert â€“ in einfachen Worten, mit Beispielcode in Python."</code>

**Antwort (gekÃ¼rzt):**
### ğŸ§  Was ist Merge Sort?
Merge Sort ist ein "Teile und Herrsche" (Divide and Conquer)-Algorithmus zum Sortieren von Listen. Das bedeutet:

Teilen: Zerteile die Liste in zwei HÃ¤lften.

Sortieren: Sortiere jede HÃ¤lfte rekursiv (also nach dem gleichen Prinzip).

ZusammenfÃ¼gen (Merge): FÃ¼ge die beiden sortierten HÃ¤lften so zusammen, dass die gesamte Liste sortiert ist.

### ğŸ“Š Beispiel: Liste [5, 3, 8, 6, 2, 7, 4, 1]
Zerlege in HÃ¤lften: 
```python 
[5, 3, 8, 6]  [2, 7, 4, 1]
``` 

Zerlege weiter, bis nur noch Einzellisten da sind:
```python 
[5], [3], [8], [6], [2], [7], [4], [1]
``` 

Jetzt Merge:
```python 
[5] + [3] â†’ [3, 5]  
[8] + [6] â†’ [6, 8]
``` 

usw., bis am Ende:
```python 
[1, 2, 3, 4, 5, 6, 7, 8]
``` 

### ğŸ§‘â€ğŸ’» Python-Code
```python
def merge_sort(arr):
    # Wenn die Liste 0 oder 1 Element hat, ist sie schon sortiert
    if len(arr) <= 1:
        return arr

    # Liste in zwei HÃ¤lften teilen
    mid = len(arr) // 2
    left_half = merge_sort(arr[:mid])
    right_half = merge_sort(arr[mid:])

    # Die sortierten HÃ¤lften zusammenfÃ¼hren
    return merge(left_half, right_half)

def merge(left, right):
    sorted_list = []
    i = j = 0

    # Solange beide Listen noch Elemente haben
    while i < len(left) and j < len(right):
        if left[i] < right[j]:
            sorted_list.append(left[i])
            i += 1
        else:
            sorted_list.append(right[j])
            j += 1

    # Restliche Elemente anhÃ¤ngen (nur eine Liste hat noch was Ã¼brig)
    sorted_list.extend(left[i:])
    sorted_list.extend(right[j:])

    return sorted_list

# Beispiel:
unsortierte_liste = [5, 3, 8, 6, 2, 7, 4, 1]
sortierte_liste = merge_sort(unsortierte_liste)
print(sortierte_liste)
```

### ğŸŸ¢ Ausgabe:
```python 
[1, 2, 3, 4, 5, 6, 7, 8]
```

### âœ… Vorteile von Merge Sort:
- Sehr effizient: immer ğ‘‚(ğ‘› log ğ‘›)
- Funktioniert auch gut bei groÃŸen Datenmengen
- Stabiler Sortieralgorithmus (Reihenfolge bleibt bei gleichen Werten erhalten)

## 3. Wie LLMs entstanden sind â€“ ein Blick zurÃ¼ck

LLMs sind das Ergebnis jahrelanger Forschung. Seit dem Transformer-Durchbruch 2017 ging es rasant voran:
- **GPT-2** (2019): ~1,5 Mrd. Parameter
- **GPT-3** (2020): 175 Mrd. Parameter
- **GPT-4** (2023): GrÃ¶ÃŸe unbekannt, aber deutlich leistungsstÃ¤rker

Parallel entstanden leistungsfÃ¤hige Open-Source-Modelle:
- LLaMA 3 (Meta), Mistral, Falcon, BLOOM u. a.

### Wie groÃŸ ist "groÃŸ"?

**GPT-3** > 86 Mrd. Neuronen im menschlichen Gehirn

Millionen $ Rechenkosten, hoher Strombedarf, COâ‚‚-AusstoÃŸ

**Aber:** GrÃ¶ÃŸe allein ist nicht alles. Architektur, DatenqualitÃ¤t und Feintuning spielen ebenso eine Rolle.

## 4. Wie LLMs unter der Haube funktionieren
### Tokenisierung

Text wird in Tokens zerlegt und in Zahlen umgewandelt. Beispiel:
```
"ChatGPT erklÃ¤rt Code." â†’ ["Chat", "G", "PT", " erklÃ¤rt", " Code", "."] 
```

### Embeddings
Tokens werden in VektorrÃ¤ume eingebettet â€“ BedeutungsnÃ¤he wird mathematisch erfassbar.

### Attention â€“ wer spricht mit wem?

Self-Attention ermÃ¶glicht KontextverstÃ¤ndnis. Beispiel:
```
1. Der Hund hat den Mann gebissen.
2. Der Mann hat den Hund gebissen.
```
Gleiche WÃ¶rter, andere Reihenfolge â€“ andere Bedeutung. Attention erkennt solche Rollenunterschiede.

### Training vs. Inferenz

- **Training:** Wochenlange Berechnungen mit riesigen Datenmengen
- **Inferenz:** Reaktion in Echtzeit auf Nutzereingaben

### Ressourcen

- Hoher Stromverbrauch, tausende GPUs, spezialisierte Rechenzentren
- Milliarden Tokens im Training

## 5.Ãœberblick: Die wichtigsten LLMs im Vergleich

| Modell      | Offenheit         | KontextgrÃ¶ÃŸe | Multimodal? | Fokus           | Quellenangabe |
|-------------|-------------------|---------------|-------------|------------------|----------------|
| ChatGPT     | ProprietÃ¤r        | Hoch          | Ja          | Vielseitigkeit   | Teilweise       |
| Gemini      | ProprietÃ¤r        | Sehr hoch     | Ja          | Fakten + Code    | Teilweise       |
| Claude      | ProprietÃ¤r        | Sehr hoch     | (Bald)      | Sicherheit       | Nein            |
| LLaMA       | Open-Weight       | Mittel        | Nein        | Forschung        | Nein            |
| Copilot     | ProprietÃ¤r        | Hoch          | Teilweise   | ProduktivitÃ¤t    | Nein            |
| Perplexity  | Teilweise offen   | Mittel        | Nein        | Webrecherche     | Ja              |
